{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS7iBPtWjF6s"
   },
   "source": [
    "# **Portfolio Project:** Web Scraping Using Scrapy\n",
    "# **Rob Boswell**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilY0AUMcjTIy"
   },
   "source": [
    "### This portfolio project shows how to create a web crawler using the Scrapy Python library. I demonstrate how to combine Scrapy with a TOR-based library to enable anonymous web scraping.\n",
    "\n",
    "<br>\n",
    "\n",
    "### I will show two related ways of scraping web text data. The second technique is generally considered better than the first:\n",
    "\n",
    "<br>\n",
    "\n",
    "### In my first code implementation, the crawler starts with a single predefined URL and follows all links found on the page, collecting paragraph text from each linked page it visits.\n",
    "\n",
    "<br>\n",
    "\n",
    "### The scraped text data is stored in a dictionary, with URLs as keys and lists of paragraph texts as values. This data is then processed to remove empty strings and standardized into a pandas DataFrame for easier analysis and presentation. This implementation demonstrates a simple way to build a web scraper for exploring web page contents anonymously.\n",
    "\n",
    "<br>\n",
    "\n",
    "### The second code implementation showcases how to scrape text data from HTML documents while cleaning the data by removing empty strings. Users can specify multiple URLs to scrape, and the code collects and processes all links found within each specified URL, scraping paragraphs from each linked page.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Each specified URL is assigned a unique ID, which is used in a pandas DataFrame to associate the collected links and scraped text with their originating URL, facilitating easier data retrieval. There are many design options for web scrapers depending on specific goals; for instance, this code could be adapted to scrape PDF files, meta data, or images instead of HTML text. E.g., for more information, see: [\"How do I scrape PDFs with Scrapy?\"](https://webscraping.ai/faq/scrapy/how-do-i-scrape-pdfs-with-scrapy).  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czem515CkblF",
    "outputId": "c9416341-3fd9-49af-e328-17f4cd7f04f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Twisted>=18.9.0 (from scrapy)\n",
      "  Downloading twisted-24.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (42.0.8)\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2.1)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m729.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (71.0.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.4)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.0)\n",
      "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=22.10.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.15.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=22.10.0->Twisted>=18.9.0->scrapy) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n",
      "Downloading Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading itemloaders-1.3.1-py3-none-any.whl (12 kB)\n",
      "Downloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
      "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Downloading twisted-24.3.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.3.0 automat-22.10.0 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.1 jmespath-1.0.1 parsel-1.9.1 protego-0.3.1 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.11.2 service-identity-24.1.0 tldextract-5.1.2 w3lib-2.2.1 zope.interface-7.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai_8PGT3pPrD"
   },
   "source": [
    "### **First Web Scraper Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzSuoSaCBZla",
    "outputId": "90d18124-b51a-4cc5-f2d9-e35f688f9cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  logrotate tor-geoipdb torsocks\n",
      "Suggested packages:\n",
      "  bsd-mailx | mailx mixmaster torbrowser-launcher socat apparmor-utils nyx obfs4proxy\n",
      "The following NEW packages will be installed:\n",
      "  logrotate tor tor-geoipdb torsocks\n",
      "0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n",
      "Need to get 2,884 kB of archives.\n",
      "After this operation, 15.5 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tor amd64 0.4.6.10-1 [1,665 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 torsocks amd64 2.3.0-3 [62.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tor-geoipdb all 0.4.6.10-1 [1,103 kB]\n",
      "Fetched 2,884 kB in 1s (2,691 kB/s)\n",
      "Selecting previously unselected package logrotate.\n",
      "(Reading database ... 123598 files and directories currently installed.)\n",
      "Preparing to unpack .../logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
      "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
      "Selecting previously unselected package tor.\n",
      "Preparing to unpack .../tor_0.4.6.10-1_amd64.deb ...\n",
      "Unpacking tor (0.4.6.10-1) ...\n",
      "Selecting previously unselected package torsocks.\n",
      "Preparing to unpack .../torsocks_2.3.0-3_amd64.deb ...\n",
      "Unpacking torsocks (2.3.0-3) ...\n",
      "Selecting previously unselected package tor-geoipdb.\n",
      "Preparing to unpack .../tor-geoipdb_0.4.6.10-1_all.deb ...\n",
      "Unpacking tor-geoipdb (0.4.6.10-1) ...\n",
      "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
      "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n",
      "Setting up tor (0.4.6.10-1) ...\n",
      "Something or somebody made /var/lib/tor disappear.\n",
      "Creating one for you again.\n",
      "Something or somebody made /var/log/tor disappear.\n",
      "Creating one for you again.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/tor.service → /lib/systemd/system/tor.service.\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up torsocks (2.3.0-3) ...\n",
      "Setting up tor-geoipdb (0.4.6.10-1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Collecting stem\n",
      "  Downloading stem-1.8.2.tar.gz (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: stem\n",
      "  Building wheel for stem (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for stem: filename=stem-1.8.2-py3-none-any.whl size=436205 sha256=b8d41257de370d5cc4637a87bdf610d7c7e40e5f020bae12c7c4261c047265e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/f3/41/2af9219109ac95b19b149169b0a8c99c714c021cb82e813498\n",
      "Successfully built stem\n",
      "Installing collected packages: stem\n",
      "Successfully installed stem-1.8.2\n",
      "Requirement already satisfied: pysocks in /usr/local/lib/python3.10/dist-packages (1.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to renew TOR connection: Socket error: 0x01: General SOCKS server failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1d4360bb8138>\", line 84, in <cell line: 84>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1d4360bb8138>\", line 84, in <cell line: 84>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1d4360bb8138>\", line 84, in <cell line: 84>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1d4360bb8138>\", line 84, in <cell line: 84>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1d4360bb8138>\", line 84, in <cell line: 84>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1d4360bb8138>\", line 84, in <cell line: 84>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1d4360bb8138>\", line 84, in <cell line: 84>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code is intentionally designed to minimize the amount of logging output. Since so many URLs are being scraped, you will still occasionally see error code.\n",
    "# Step 1: Install necessary packages and start TOR\n",
    "!apt-get install -y tor\n",
    "!pip install stem\n",
    "!pip install pysocks\n",
    "\n",
    "import time\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "import subprocess\n",
    "import socks\n",
    "import socket\n",
    "\n",
    "# Step 2: Create the torrc file\n",
    "torrc_content = \"\"\"\n",
    "ControlPort 9051\n",
    "CookieAuthentication 0\n",
    "SocksPort 9050\n",
    "Log notice file /var/log/tor/notices.log\n",
    "\"\"\"\n",
    "with open('torrc', 'w') as f:\n",
    "    f.write(torrc_content)\n",
    "\n",
    "# Step 3: Start TOR using subprocess and wait for it to be ready\n",
    "tor_process = subprocess.Popen(['tor', '-f', 'torrc'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Give TOR some time to start\n",
    "time.sleep(20)\n",
    "\n",
    "# Step 4: Renew TOR identity\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port=9051) as controller:\n",
    "        controller.authenticate()\n",
    "        controller.signal(Signal.NEWNYM)\n",
    "\n",
    "# Step 5: Completely Disable Logging\n",
    "import logging\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "# Disable logging for all levels\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# Step 6: Scrapy Spider\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Set up SOCKS5 proxy for Scrapy using PySocks\n",
    "socks.set_default_proxy(socks.SOCKS5, \"127.0.0.1\", 9050)\n",
    "socket.socket = socks.socksocket\n",
    "\n",
    "# Initialize the dictionary outside of the Spider class\n",
    "data_link_dict = dict()\n",
    "\n",
    "# Create the Spider class\n",
    "class DCChapterSpider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        try:\n",
    "            renew_connection()  # Renew TOR identity before starting requests\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to renew TOR connection: {e}\")  # Use print instead of log for errors\n",
    "        yield scrapy.Request(url='https://en.wikipedia.org/wiki/Web_scraping',\n",
    "                             callback=self.parse1)\n",
    "\n",
    "    def parse1(self, response):\n",
    "        links = response.xpath('//a/@href').extract()\n",
    "        # Process links without logging\n",
    "        for link in links:\n",
    "            absolute_url = response.urljoin(link)\n",
    "            yield response.follow(url=absolute_url, callback=self.parse2)\n",
    "\n",
    "    def parse2(self, response):\n",
    "        # Correct XPath selector for extracting paragraph text\n",
    "        par_text = response.xpath('//p/text()').extract()\n",
    "        par_text_strip = [t.strip() for t in par_text]\n",
    "        # Use response.url as the key for the dictionary\n",
    "        data_link_dict[response.url] = par_text_strip\n",
    "        # No logging needed\n",
    "\n",
    "# Configure Scrapy to use the SOCKS proxy\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCChapterSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zJsUnBHL6RL"
   },
   "outputs": [],
   "source": [
    "def print_dict_head(d, n=5):\n",
    "    \"\"\"Print the first n items of a dictionary.\"\"\"\n",
    "    # Use enumerate to limit the number of items printed\n",
    "    for i, (key, value) in enumerate(d.items()):\n",
    "        if i >= n:\n",
    "            break\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtPZbWjcL-ki",
    "outputId": "add081dc-b473-40c2-d7fd-05e9dbe619ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Special:MyTalk: ['People on Wikipedia can use this', 'to post a public message about edits made from the IP address you are currently using.', 'Many IP addresses change periodically, and are often shared by several people. You may', 'or', 'to avoid future confusion with other logged out users. Creating an account also hides your IP address.']\n",
      "https://en.wikipedia.org/wiki/Help:Introduction: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "https://en.wikipedia.org/wiki/Special:RecentChanges: ['This is a list of recent changes to Wikipedia.']\n",
      "https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard: ['Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand', 'and the', 'before proceeding.', '', 'Uploads to', '', '', '', 'Uploads locally to the English Wikipedia; must comply with the', 'criteria', '', '', 'Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain', 'page to upload files to the English Wikipedia without JavaScript.', '', 'Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please', 'and then try again.', '', 'Sorry, in order to upload files on the English Wikipedia, you need to have a', '. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.', 'You may already be able to upload files on the', \", but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.\", \"if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at\", '.', '', '', '', '', '', '', '', '', '', '', 'Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for', 'and', '. Your filename has been modified to avoid these. Please check if it is okay now.', \"The filename you chose seems to be very short, or overly generic. Please don't use:\", '', 'If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.', 'This should not be done, except in very rare exceptional cases.', \"Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.\", '', \"If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:\", 'It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.', '', 'Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the', '.\\nFiles uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too.', '', '', 'However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.', 'Please note that by \"entirely self-made\" we really mean just that.', 'use this section for any of the following:', 'Editors who falsely declare such items as their \"own work\"', '.', 'Use this', 'if there is an', 'in the source.', 'The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.', \"If the source website doesn't say so explicitly, please\", '.', 'means that nobody owns any copyrights on this work. It does', 'mean simply that it is freely viewable somewhere on the web or that it has been widely used by others.', 'This is', \"for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then\", '', 'Please remember that you will need to demonstrate that:', 'This file will be used in the following article:', '', '', '– article okay.', '', '', 'The article', 'could not be found.', 'Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.', 'If this is an article you are only planning to write, please write it first and upload the file afterwards.', '', '', 'The page', 'is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.', 'Please upload this file only if it is going to be used in an actual article.', \"If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.\", '', '', 'The page', 'is not a real article, but a disambiguation page pointing to a number of other pages.', 'Please check and enter the exact title of the actual target article you meant.', 'If neither of these two statements applies, then please', '', 'This section is', 'for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.', 'In view of this, please explain how the use of this file will be minimal.', \"Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:\", '', \"Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is\", 'to be fully-copyrighted unless shown otherwise; the burden is on the uploader.', \"In particular, please don't upload:\", 'If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at', '. Thank you.', 'This is the data that will be submitted to upload:', '', '', '', 'Your file is being uploaded.', 'This might take a minute or two, depending on the size of the file and the speed of your internet connection.', 'Once uploading is completed, you will find your new file at this link:', '', 'Your file has been uploaded successfully and can now be found here:', '', 'Please follow the link and check that the image description page has all the information you meant to include.', 'If you want to change the description, just go to the image page, click the \"edit\" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.', 'To insert this file into an article, you may want to use code similar to the following:', 'If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the \":\" after the initial brackets!):', 'See', 'for more detailed help on how to insert and position images in pages.', 'Please leave your feedback, comments, bug reports or suggestions on the', '.', '']\n",
      "https://en.wikipedia.org/wiki/Special:Search: []\n"
     ]
    }
   ],
   "source": [
    "# By printing just the first part of the dictionary, we can see there are many empty strings that have been scraped. We should delete the empty strings\n",
    "print_dict_head(data_link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "rfMNwTkOMY2v",
    "outputId": "081e543e-8875-40eb-81cc-00c0a4657d76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6617e407-81c4-4c68-9c32-c1d5879595f7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>https://en.wikipedia.org/wiki/Special:MyTalk</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Help:Introduction</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Special:RecentChanges</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Special:Search</th>\n",
       "      <th>https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=Web+scraping</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Help:Contents</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Wikipedia:Contact_us</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Wikipedia:About</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Wikipedia:Community_portal</th>\n",
       "      <th>...</th>\n",
       "      <th>https://en.wikipedia.org/wiki/Main_Page</th>\n",
       "      <th>https://ar.wikipedia.org/wiki/%D8%AA%D8%AC%D8%B1%D9%8A%D9%81_%D9%88%D9%8A%D8%A8</th>\n",
       "      <th>https://www.eff.org/cases/facebook-v-power-ventures</th>\n",
       "      <th>https://cs.wikipedia.org/wiki/Web_scraping</th>\n",
       "      <th>https://web.archive.org/web/20071012005033/http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf</th>\n",
       "      <th>https://www.semanticscholar.org/paper/Joint-optimization-of-wrapper-generation-and-Zheng-Song/61db194fc4693b002d507c6f027beeefef6ae3e7?p2df</th>\n",
       "      <th>https://www.techdirt.com/2009/06/10/can-scraping-non-infringing-content-become-copyright-infringement-because-of-how-scrapers-work/</th>\n",
       "      <th>https://web.archive.org/web/20191203113701/https://www.lloyds.com/~/media/5880dae185914b2487bed7bd63b96286.ashx</th>\n",
       "      <th>https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_1babb80b-28df-41ba-9864-b630fdb67946</th>\n",
       "      <th>https://web.archive.org/web/20120624103316/http://www.lkshields.ie/htmdocs/publications/newsletters/update26/update26_03.htm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People on Wikipedia can use this</td>\n",
       "      <td></td>\n",
       "      <td>This is a list of recent changes to Wikipedia.</td>\n",
       "      <td>Thank you for offering to contribute an image ...</td>\n",
       "      <td></td>\n",
       "      <td>edits</td>\n",
       "      <td>This page provides</td>\n",
       "      <td>How to report a problem with an article, or fi...</td>\n",
       "      <td>is a</td>\n",
       "      <td>This page provides a listing of current collab...</td>\n",
       "      <td>...</td>\n",
       "      <td>is a</td>\n",
       "      <td>(</td>\n",
       "      <td>EFF has urged a San Francisco federal court an...</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Earlier this year, we couldn’t figure out how ...</td>\n",
       "      <td>History is littered with hundreds of conflicts...</td>\n",
       "      <td>We, TechCrunch, are part of the</td>\n",
       "      <td>Website owners often have to contend with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to post a public message about edits made from...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>and the</td>\n",
       "      <td></td>\n",
       "      <td>articles</td>\n",
       "      <td>.</td>\n",
       "      <td>Problems with articles about you, your company...</td>\n",
       "      <td>that anyone can edit, and</td>\n",
       "      <td>? See the</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>:</td>\n",
       "      <td>Power Ventures was a company that allowed user...</td>\n",
       "      <td>nebo</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>. Power.com tried to aggregate various social ...</td>\n",
       "      <td>The main site for Archive Team is at</td>\n",
       "      <td>family of brands.</td>\n",
       "      <td>(26 February 2010). However, \\n              i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Many IP addresses change periodically, and are...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>before proceeding.</td>\n",
       "      <td></td>\n",
       "      <td>recent contributors</td>\n",
       "      <td>You can also search Wikipedia's help pages usi...</td>\n",
       "      <td>How to copy Wikipedia's information, donate yo...</td>\n",
       "      <td>.</td>\n",
       "      <td>page or</td>\n",
       "      <td>...</td>\n",
       "      <td>. Taking the name from a local landmark, forme...</td>\n",
       "      <td>)‏ هي تقنية استخراج البيانات من مواقع</td>\n",
       "      <td>(CFAA) and the California state CFAA equivalen...</td>\n",
       "      <td>označují způsob získávání</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>points us to</td>\n",
       "      <td>and contains up to the date information on var...</td>\n",
       "      <td>If you do not want us and our partners to use ...</td>\n",
       "      <td>The Ryanair case concerned a claim by Ryanair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>or</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Uploads to</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>or the</td>\n",
       "      <td>Find out about the process, how to donate, and...</td>\n",
       "      <td>is to benefit readers by presenting informatio...</td>\n",
       "      <td>for everything you need to know to get started...</td>\n",
       "      <td>...</td>\n",
       "      <td>, and the building's two other floors were use...</td>\n",
       "      <td>عن طريق برامج مخصصة مثل برامج محاكة تصفح الأشخ...</td>\n",
       "      <td>, the federal law that prohibits sending comme...</td>\n",
       "      <td>z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, and separately</td>\n",
       "      <td>This collection contains the output of many Ar...</td>\n",
       "      <td>'.</td>\n",
       "      <td>Mr Justice Hanna's decision relates only to a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to avoid future confusion with other logged ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Uploads locally to the English Wikipedia; must...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td>If you're a member of the press looking to con...</td>\n",
       "      <td>. Hosted by the</td>\n",
       "      <td>of interest, see the</td>\n",
       "      <td>...</td>\n",
       "      <td>. By 2013, persistent high demand for Blackroc...</td>\n",
       "      <td>متكامل، مثل</td>\n",
       "      <td>In February 2012, the district court found Pow...</td>\n",
       "      <td>. Spočívá v extrahování dat umístěných na webo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>of the ruling. Neuberger states the following:</td>\n",
       "      <td>, providing a path back to lost websites and w...</td>\n",
       "      <td>If you would like to customise your choices, c...</td>\n",
       "      <td>In any dispute, there is an initial issue that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 259 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6617e407-81c4-4c68-9c32-c1d5879595f7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6617e407-81c4-4c68-9c32-c1d5879595f7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6617e407-81c4-4c68-9c32-c1d5879595f7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ee6df04a-bc5f-41f5-9aa6-98d998e9f409\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee6df04a-bc5f-41f5-9aa6-98d998e9f409')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ee6df04a-bc5f-41f5-9aa6-98d998e9f409 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        https://en.wikipedia.org/wiki/Special:MyTalk  \\\n",
       "0                   People on Wikipedia can use this   \n",
       "1  to post a public message about edits made from...   \n",
       "2  Many IP addresses change periodically, and are...   \n",
       "3                                                 or   \n",
       "4  to avoid future confusion with other logged ou...   \n",
       "\n",
       "  https://en.wikipedia.org/wiki/Help:Introduction  \\\n",
       "0                                                   \n",
       "1                                                   \n",
       "2                                                   \n",
       "3                                                   \n",
       "4                                                   \n",
       "\n",
       "  https://en.wikipedia.org/wiki/Special:RecentChanges  \\\n",
       "0     This is a list of recent changes to Wikipedia.    \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                                                       \n",
       "4                                                       \n",
       "\n",
       "  https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard  \\\n",
       "0  Thank you for offering to contribute an image ...           \n",
       "1                                            and the           \n",
       "2                                 before proceeding.           \n",
       "3                                         Uploads to           \n",
       "4  Uploads locally to the English Wikipedia; must...           \n",
       "\n",
       "  https://en.wikipedia.org/wiki/Special:Search  \\\n",
       "0                                                \n",
       "1                                                \n",
       "2                                                \n",
       "3                                                \n",
       "4                                                \n",
       "\n",
       "  https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Web+scraping  \\\n",
       "0                                              edits                                       \n",
       "1                                           articles                                       \n",
       "2                                recent contributors                                       \n",
       "3                                                                                          \n",
       "4                                                                                          \n",
       "\n",
       "         https://en.wikipedia.org/wiki/Help:Contents  \\\n",
       "0                                 This page provides   \n",
       "1                                                  .   \n",
       "2  You can also search Wikipedia's help pages usi...   \n",
       "3                                             or the   \n",
       "4                                                  .   \n",
       "\n",
       "  https://en.wikipedia.org/wiki/Wikipedia:Contact_us  \\\n",
       "0  How to report a problem with an article, or fi...   \n",
       "1  Problems with articles about you, your company...   \n",
       "2  How to copy Wikipedia's information, donate yo...   \n",
       "3  Find out about the process, how to donate, and...   \n",
       "4  If you're a member of the press looking to con...   \n",
       "\n",
       "       https://en.wikipedia.org/wiki/Wikipedia:About  \\\n",
       "0                                               is a   \n",
       "1                          that anyone can edit, and   \n",
       "2                                                  .   \n",
       "3  is to benefit readers by presenting informatio...   \n",
       "4                                    . Hosted by the   \n",
       "\n",
       "  https://en.wikipedia.org/wiki/Wikipedia:Community_portal  ...  \\\n",
       "0  This page provides a listing of current collab...        ...   \n",
       "1                                          ? See the        ...   \n",
       "2                                            page or        ...   \n",
       "3  for everything you need to know to get started...        ...   \n",
       "4                               of interest, see the        ...   \n",
       "\n",
       "             https://en.wikipedia.org/wiki/Main_Page  \\\n",
       "0                                               is a   \n",
       "1                                                 in   \n",
       "2  . Taking the name from a local landmark, forme...   \n",
       "3  , and the building's two other floors were use...   \n",
       "4  . By 2013, persistent high demand for Blackroc...   \n",
       "\n",
       "  https://ar.wikipedia.org/wiki/%D8%AA%D8%AC%D8%B1%D9%8A%D9%81_%D9%88%D9%8A%D8%A8  \\\n",
       "0                                                  (                                \n",
       "1                                                  :                                \n",
       "2              )‏ هي تقنية استخراج البيانات من مواقع                                \n",
       "3  عن طريق برامج مخصصة مثل برامج محاكة تصفح الأشخ...                                \n",
       "4                                        متكامل، مثل                                \n",
       "\n",
       "  https://www.eff.org/cases/facebook-v-power-ventures  \\\n",
       "0  EFF has urged a San Francisco federal court an...    \n",
       "1  Power Ventures was a company that allowed user...    \n",
       "2  (CFAA) and the California state CFAA equivalen...    \n",
       "3  , the federal law that prohibits sending comme...    \n",
       "4  In February 2012, the district court found Pow...    \n",
       "\n",
       "          https://cs.wikipedia.org/wiki/Web_scraping  \\\n",
       "0                                                  ,   \n",
       "1                                               nebo   \n",
       "2                          označují způsob získávání   \n",
       "3                                                  z   \n",
       "4  . Spočívá v extrahování dat umístěných na webo...   \n",
       "\n",
       "  https://web.archive.org/web/20071012005033/http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf  \\\n",
       "0                                                                                                                                         \n",
       "1                                                                                                                                         \n",
       "2                                                                                                                                         \n",
       "3                                                                                                                                         \n",
       "4                                                                                                                                         \n",
       "\n",
       "  https://www.semanticscholar.org/paper/Joint-optimization-of-wrapper-generation-and-Zheng-Song/61db194fc4693b002d507c6f027beeefef6ae3e7?p2df  \\\n",
       "0                                                                                                                                               \n",
       "1                                                                                                                                               \n",
       "2                                                                                                                                               \n",
       "3                                                                                                                                               \n",
       "4                                                                                                                                               \n",
       "\n",
       "  https://www.techdirt.com/2009/06/10/can-scraping-non-infringing-content-become-copyright-infringement-because-of-how-scrapers-work/  \\\n",
       "0  Earlier this year, we couldn’t figure out how ...                                                                                    \n",
       "1  . Power.com tried to aggregate various social ...                                                                                    \n",
       "2                                       points us to                                                                                    \n",
       "3                                   , and separately                                                                                    \n",
       "4     of the ruling. Neuberger states the following:                                                                                    \n",
       "\n",
       "  https://web.archive.org/web/20191203113701/https://www.lloyds.com/~/media/5880dae185914b2487bed7bd63b96286.ashx  \\\n",
       "0  History is littered with hundreds of conflicts...                                                                \n",
       "1               The main site for Archive Team is at                                                                \n",
       "2  and contains up to the date information on var...                                                                \n",
       "3  This collection contains the output of many Ar...                                                                \n",
       "4  , providing a path back to lost websites and w...                                                                \n",
       "\n",
       "  https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_1babb80b-28df-41ba-9864-b630fdb67946  \\\n",
       "0                    We, TechCrunch, are part of the                                                        \n",
       "1                                  family of brands.                                                        \n",
       "2  If you do not want us and our partners to use ...                                                        \n",
       "3                                                 '.                                                        \n",
       "4  If you would like to customise your choices, c...                                                        \n",
       "\n",
       "  https://web.archive.org/web/20120624103316/http://www.lkshields.ie/htmdocs/publications/newsletters/update26/update26_03.htm  \n",
       "0  Website owners often have to contend with the ...                                                                            \n",
       "1  (26 February 2010). However, \\n              i...                                                                            \n",
       "2  The Ryanair case concerned a claim by Ryanair ...                                                                            \n",
       "3  Mr Justice Hanna's decision relates only to a ...                                                                            \n",
       "4  In any dispute, there is an initial issue that...                                                                            \n",
       "\n",
       "[5 rows x 259 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Clean the dictionary by removing empty strings\n",
    "cleaned_dict = {k: [item for item in v if item] for k, v in data_link_dict.items()}\n",
    "\n",
    "# Find the maximum number of paragraphs for any URL to standardize the DataFrame\n",
    "max_length = max(len(v) for v in cleaned_dict.values())\n",
    "\n",
    "# Create a standardized dictionary with lists of equal length\n",
    "standardized_dict = {k: v + [''] * (max_length - len(v)) for k, v in cleaned_dict.items()}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(standardized_dict, orient='index').transpose()\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DY_BtTNmMr9e",
    "outputId": "81e94a37-61a4-4faf-99f6-ad330889e994"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044, 259)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that the total number of URL links that were scraped is 259, and that the largest number of paragraphs scraped from at least one of these links was 1044.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdhBRStMJ8h1"
   },
   "source": [
    "#### If you prefer seeing logging output, below is a commented out version of the same code that implements logging to detect more errors, as well as to see which sites have been scraped, how many URL links were found on a given page, how many paragraphs were scraped on a given site, and the exact contents of the paragraphs that were scraped:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy5yRuCqJ7Ro"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Step 1: Install necessary packages and start TOR\n",
    "!apt-get install -y tor\n",
    "!pip install stem\n",
    "!pip install pysocks\n",
    "\n",
    "import time\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "import subprocess\n",
    "import os\n",
    "import socks\n",
    "import socket\n",
    "\n",
    "# Step 2: Create the torrc file\n",
    "torrc_content = \"\"\"\n",
    "ControlPort 9051\n",
    "CookieAuthentication 0\n",
    "SocksPort 9050\n",
    "Log notice file /var/log/tor/notices.log\n",
    "\"\"\"\n",
    "with open('torrc', 'w') as f:\n",
    "    f.write(torrc_content)\n",
    "\n",
    "# Step 3: Start TOR using subprocess and wait for it to be ready\n",
    "tor_process = subprocess.Popen(['tor', '-f', 'torrc'])\n",
    "\n",
    "# Give TOR some time to start\n",
    "time.sleep(20)\n",
    "\n",
    "# Step 4: Renew TOR identity\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port=9051) as controller:\n",
    "        controller.authenticate()\n",
    "        controller.signal(Signal.NEWNYM)\n",
    "\n",
    "# Step 5: Scrapy Spider\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Set up SOCKS5 proxy for Scrapy using PySocks\n",
    "socks.set_default_proxy(socks.SOCKS5, \"127.0.0.1\", 9050)\n",
    "socket.socket = socks.socksocket\n",
    "\n",
    "# Initialize the dictionary outside of the Spider class\n",
    "data_link_dict = dict()\n",
    "\n",
    "# Create the Spider class\n",
    "class DCChapterSpider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        try:\n",
    "            renew_connection()  # Renew TOR identity before starting requests\n",
    "        except Exception as e:\n",
    "            self.log(f\"Failed to renew TOR connection: {e}\")\n",
    "        yield scrapy.Request(url='https://en.wikipedia.org/wiki/Web_scraping',\n",
    "                             callback=self.parse1)\n",
    "\n",
    "    def parse1(self, response):\n",
    "        links = response.xpath('//a/@href').extract()\n",
    "        self.log(f\"Found {len(links)} links on the page\")\n",
    "        for link in links:\n",
    "            absolute_url = response.urljoin(link)\n",
    "            yield response.follow(url=absolute_url, callback=self.parse2)\n",
    "\n",
    "    def parse2(self, response):\n",
    "        # Correct XPath selector for extracting paragraph text\n",
    "        par_text = response.xpath('//p/text()').extract()\n",
    "        self.log(f\"Processing URL: {response.url}\")\n",
    "        self.log(f\"Found {len(par_text)} paragraphs on the page\")\n",
    "        par_text_strip = [t.strip() for t in par_text]\n",
    "        # Use response.url as the key for the dictionary\n",
    "        data_link_dict[response.url] = par_text_strip\n",
    "        self.log(f\"Stored data for URL: {response.url}\")\n",
    "        self.log(f\"Current data_link_dict: {data_link_dict}\")\n",
    "\n",
    "# Configure Scrapy to use the SOCKS proxy\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCChapterSpider)\n",
    "process.start()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pNpwPDQ0IiC"
   },
   "source": [
    "### **Second Web Scraper Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHnyVGSKdxZC",
    "outputId": "0f3936e8-0172-45ce-e0e1-22ada4a8a789"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to renew TOR connection: Socket error: 0x01: General SOCKS server failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-380f46810f26>\", line 113, in <cell line: 113>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-380f46810f26>\", line 113, in <cell line: 113>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-380f46810f26>\", line 113, in <cell line: 113>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-380f46810f26>\", line 113, in <cell line: 113>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-380f46810f26>\", line 113, in <cell line: 113>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n",
      "Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-380f46810f26>\", line 113, in <cell line: 113>\n",
      "    process.start()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 429, in start\n",
      "    reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 695, in run\n",
      "    self.mainLoop()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 705, in mainLoop\n",
      "    self.runUntilCurrent()\n",
      "--- <exception caught here> ---\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\", line 1090, in runUntilCurrent\n",
      "    call.func(*call.args, **call.kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 448, in resolveAddress\n",
      "    self._setRealAddress(self.addr)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 469, in _setRealAddress\n",
      "    self.doConnect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/tcp.py\", line 592, in doConnect\n",
      "    connectResult = se.args[0]\n",
      "builtins.IndexError: tuple index out of range\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: df_url_1\n",
      "                                   origin_url  \\\n",
      "0  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "1  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "2  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "3  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "4  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "\n",
      "                                         scraped_url  \\\n",
      "0       https://en.wikipedia.org/wiki/Special:MyTalk   \n",
      "1  https://en.wikipedia.org/w/index.php?title=Spe...   \n",
      "2  https://en.wikipedia.org/wiki/Wikipedia:File_u...   \n",
      "3       https://en.wikipedia.org/wiki/Special:Search   \n",
      "4    https://en.wikipedia.org/wiki/Help:Introduction   \n",
      "\n",
      "                                                text  \n",
      "0  This user is currently blocked.\\nThe latest bl...  \n",
      "1                 edits articles recent contributors  \n",
      "2  Thank you for offering to contribute an image ...  \n",
      "3                                                     \n",
      "4                                                     \n",
      "DataFrame: df_url_2\n",
      "                                    origin_url  \\\n",
      "0  https://en.wikipedia.org/wiki/Data_scraping   \n",
      "1  https://en.wikipedia.org/wiki/Data_scraping   \n",
      "2  https://en.wikipedia.org/wiki/Data_scraping   \n",
      "3  https://en.wikipedia.org/wiki/Data_scraping   \n",
      "4  https://en.wikipedia.org/wiki/Data_scraping   \n",
      "\n",
      "                                         scraped_url  \\\n",
      "0  https://en.wikipedia.org/wiki/Category:All_art...   \n",
      "1  https://en.wikipedia.org/wiki/Category:Data_pr...   \n",
      "2  https://en.wikipedia.org/wiki/Category:Article...   \n",
      "3       https://en.wikipedia.org/wiki/Site_isolation   \n",
      "4  https://en.wikipedia.org/wiki/Security_informa...   \n",
      "\n",
      "                                                text  \n",
      "0  Articles in category contain all recorded . Th...  \n",
      "1  This category has the following 6 subcategorie...  \n",
      "2  This category combines all articles needing ad...  \n",
      "3  is a feature in certain that allow sites to be...  \n",
      "4  ( ) is a field within the field of , where sof...  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install necessary packages and start TOR\n",
    "!apt-get install -y tor > /dev/null 2>&1\n",
    "!pip install stem > /dev/null 2>&1\n",
    "!pip install pysocks > /dev/null 2>&1\n",
    "!pip install pandas > /dev/null 2>&1\n",
    "!pip install scrapy > /dev/null 2>&1\n",
    "\n",
    "import time\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "import subprocess\n",
    "import socks\n",
    "import socket\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Create the torrc file\n",
    "torrc_content = \"\"\"\n",
    "ControlPort 9051\n",
    "CookieAuthentication 0\n",
    "SocksPort 9050\n",
    "Log notice file /var/log/tor/notices.log\n",
    "\"\"\"\n",
    "with open('torrc', 'w') as f:\n",
    "    f.write(torrc_content)\n",
    "\n",
    "# Step 3: Start TOR using subprocess and wait for it to be ready\n",
    "tor_process = subprocess.Popen(['tor', '-f', 'torrc'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Give TOR some time to start\n",
    "time.sleep(20)\n",
    "\n",
    "# Step 4: Renew TOR identity\n",
    "def renew_connection():\n",
    "    try:\n",
    "        with Controller.from_port(port=9051) as controller:\n",
    "            controller.authenticate()\n",
    "            controller.signal(Signal.NEWNYM)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to renew TOR connection: {e}\")\n",
    "\n",
    "# Step 5: Create a list of user predetermined URLs to scrape\n",
    "urls_to_scrape = [\n",
    "    'https://en.wikipedia.org/wiki/Web_scraping',\n",
    "    'https://en.wikipedia.org/wiki/Data_scraping',\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Assign unique IDs to URLs\n",
    "url_id_map = {url: f\"url_{i+1}\" for i, url in enumerate(urls_to_scrape)}\n",
    "\n",
    "# Step 6: Suppress Scrapy's Detailed Logging\n",
    "import logging\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "# Disable logging for all levels\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# Disable all logging\n",
    "configure_logging(install_root_handler=False)\n",
    "logging.getLogger('scrapy').propagate = False\n",
    "\n",
    "# Step 7: Scrapy Spider\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Set up SOCKS5 proxy for Scrapy using PySocks\n",
    "socks.set_default_proxy(socks.SOCKS5, \"127.0.0.1\", 9050)\n",
    "socket.socket = socks.socksocket\n",
    "\n",
    "# Initialize the dictionary outside of the Spider class\n",
    "data_dicts = {}\n",
    "\n",
    "# Create the Spider class\n",
    "class DCChapterSpider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        renew_connection()  # Renew TOR identity before starting requests\n",
    "        for url, unique_id in url_id_map.items():\n",
    "            yield scrapy.Request(url=url, callback=self.parse1, meta={'unique_id': unique_id, 'origin_url': url})\n",
    "\n",
    "\n",
    "    def parse1(self, response):\n",
    "        unique_id = response.meta['unique_id']\n",
    "        origin_url = response.meta['origin_url']\n",
    "        links = response.xpath('//a/@href').extract()\n",
    "        for link in links:\n",
    "            absolute_url = response.urljoin(link)\n",
    "            # Ensure to follow the link even if it appears similar to the origin_url\n",
    "            if absolute_url != origin_url:\n",
    "                yield scrapy.Request(url=absolute_url, callback=self.parse2, meta={'unique_id': unique_id, 'scraped_url': absolute_url, 'origin_url': origin_url})\n",
    "\n",
    "\n",
    "    def parse2(self, response):\n",
    "        unique_id = response.meta['unique_id']\n",
    "        scraped_url = response.meta['scraped_url']\n",
    "        origin_url = response.meta['origin_url']\n",
    "        # Correct XPath selector for extracting paragraph text\n",
    "        par_text = response.xpath('//p/text()').extract()\n",
    "        par_text_strip = [t.strip() for t in par_text if t.strip()]  # Remove empty strings\n",
    "        # Store the data in the dictionary with unique_id as the key\n",
    "        if unique_id not in data_dicts:\n",
    "            data_dicts[unique_id] = []\n",
    "        data_dicts[unique_id].append({\n",
    "            'origin_url': origin_url,\n",
    "            'scraped_url': scraped_url,\n",
    "            'text': ' '.join(par_text_strip)  # Concatenate all paragraph text into a single string\n",
    "        })\n",
    "\n",
    "# Configure Scrapy to use the SOCKS proxy\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCChapterSpider)\n",
    "process.start()\n",
    "\n",
    "# Step 8: Process the collected data and convert each dictionary to a DataFrame\n",
    "dataframes = {}\n",
    "\n",
    "for unique_id, entries in data_dicts.items():\n",
    "    # Create a DataFrame from the list of entries\n",
    "    df = pd.DataFrame(entries)\n",
    "    # Store the DataFrame with unique ID in the name\n",
    "    dataframes[f\"df_{unique_id}\"] = df\n",
    "\n",
    "# Step 9: Display the DataFrames\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"DataFrame: {df_name}\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFrrlL2xl4R4"
   },
   "outputs": [],
   "source": [
    "df_url_1 = dataframes['df_url_1']\n",
    "df_url_2 = dataframes['df_url_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4HQYZRHil5wB",
    "outputId": "c1466a47-03de-4388-b2ea-1247110885fb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_url_1\",\n  \"rows\": 238,\n  \"fields\": [\n    {\n      \"column\": \"origin_url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://en.wikipedia.org/wiki/Web_scraping\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"scraped_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 238,\n        \"samples\": [\n          \"https://en.wikipedia.org/wiki/Electronic_Frontier_Foundation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 199,\n        \"samples\": [\n          \"( : S\\u00f8- og Handelsretten) is a specialized with jurisdiction over cases involving and . It was founded in 1861. It has a civil division, focusing on business cases, and a bankruptcy division.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_url_1"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-b60affdd-177c-4d88-a816-736e93e79295\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_url</th>\n",
       "      <th>scraped_url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Special:MyTalk</td>\n",
       "      <td>This user is currently blocked.\\nThe latest bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Spe...</td>\n",
       "      <td>edits articles recent contributors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:File_u...</td>\n",
       "      <td>Thank you for offering to contribute an image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Special:Search</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Help:Introduction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b60affdd-177c-4d88-a816-736e93e79295')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b60affdd-177c-4d88-a816-736e93e79295 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b60affdd-177c-4d88-a816-736e93e79295');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-bc45a521-712a-40e0-9a91-8e83a1fc570e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc45a521-712a-40e0-9a91-8e83a1fc570e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-bc45a521-712a-40e0-9a91-8e83a1fc570e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                   origin_url  \\\n",
       "0  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "1  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "2  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "3  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "4  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "\n",
       "                                         scraped_url  \\\n",
       "0       https://en.wikipedia.org/wiki/Special:MyTalk   \n",
       "1  https://en.wikipedia.org/w/index.php?title=Spe...   \n",
       "2  https://en.wikipedia.org/wiki/Wikipedia:File_u...   \n",
       "3       https://en.wikipedia.org/wiki/Special:Search   \n",
       "4    https://en.wikipedia.org/wiki/Help:Introduction   \n",
       "\n",
       "                                                text  \n",
       "0  This user is currently blocked.\\nThe latest bl...  \n",
       "1                 edits articles recent contributors  \n",
       "2  Thank you for offering to contribute an image ...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZ5vJQ3gupob",
    "outputId": "bf48d8d4-6aeb-4aa6-a551-99b8ad351bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   origin_url  \\\n",
      "0  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "1  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "2  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "5  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "6  https://en.wikipedia.org/wiki/Web_scraping   \n",
      "\n",
      "                                         scraped_url  \\\n",
      "0       https://en.wikipedia.org/wiki/Special:MyTalk   \n",
      "1  https://en.wikipedia.org/w/index.php?title=Spe...   \n",
      "2  https://en.wikipedia.org/wiki/Wikipedia:File_u...   \n",
      "5  https://en.wikipedia.org/wiki/Special:MyContri...   \n",
      "6  https://en.wikipedia.org/wiki/Wikipedia:Contac...   \n",
      "\n",
      "                                                text  \n",
      "0  This user is currently blocked.\\nThe latest bl...  \n",
      "1                 edits articles recent contributors  \n",
      "2  Thank you for offering to contribute an image ...  \n",
      "5  This IP address is currently blocked.\\nThe lat...  \n",
      "6  How to report a problem with an article, or fi...  \n"
     ]
    }
   ],
   "source": [
    "for key, df in dataframes.items():\n",
    "    # Strip leading and trailing whitespace from the 'text' column\n",
    "    df['text'] = df['text'].str.strip()\n",
    "\n",
    "    # Remove rows where 'text' is empty\n",
    "    df = df[df['text'] != '']\n",
    "\n",
    "    # Save the cleaned DataFrame back to the dictionary\n",
    "    dataframes[key] = df\n",
    "\n",
    "# Example to display the head of a cleaned DataFrame\n",
    "print(dataframes['df_url_1'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZglUJDXuvXo"
   },
   "outputs": [],
   "source": [
    "df_url_1 = dataframes['df_url_1']\n",
    "df_url_2 = dataframes['df_url_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2pNfj8FnuxMX",
    "outputId": "eab74dae-a324-4049-b880-69e4d22fbd03"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_url_1\",\n  \"rows\": 223,\n  \"fields\": [\n    {\n      \"column\": \"origin_url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://en.wikipedia.org/wiki/Web_scraping\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"scraped_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 223,\n        \"samples\": [\n          \"https://en.wikipedia.org/wiki/Special:RecentChanges\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 198,\n        \"samples\": [\n          \"An is a form of that controls or of an application or service. It operates by monitoring and blocking communications based on a configured policy, generally with predefined rule sets to choose from. The two primary categories of application firewalls are and . of , at , and described a third-generation firewall known as an application layer firewall. Marcus Ranum's work, based on the firewall created by , , and Jeff Mogul, spearheaded the creation of the first commercial product. The product was released by DEC, named the DEC SEAL by - Secure External Access Link. DEC's first major sale was on June 13, 1991, to Dupont. Under a broader DARPA contract at TIS, Marcus Ranum, Wei Xu, and Peter Churchyard developed the Firewall Toolkit (FWTK) and made it freely available under license in October 1993. The purposes for releasing the freely available, not for commercial use, FWTK were: to demonstrate, via the software, documentation, and methods used, how a company with (at the time) 11 years experience in formal security methods, and individuals with firewall experience, developed firewall software; to create a common base of very good firewall software for others to build on (so people did not have to continue to \\\"roll their own\\\" from scratch); to \\\"raise the bar\\\" of firewall software being used. However, FWTK was a basic application proxy requiring the user interactions. In 1994, Wei Xu extended the FWTK with the Kernel enhancement of IP stateful filter and socket transparent. This was the first transparent firewall, known as the inception of , beyond a traditional application proxy ( ), released as the commercial product known as Gauntlet firewall. Gauntlet firewall was rated one of the top application firewalls from 1995 until 1998, the year it was acquired by Network Associates Inc, (NAI). Network Associates continued to claim that Gauntlet was the \\\"worlds most secure firewall\\\" but in May 2000, security researcher discovered a large vulnerability in the firewall, allowing remote access to the operating system and bypassing the security controls. discovered a second vulnerability a year later, effectively ending Gauntlet firewalls' security dominance. filtering operates at a higher level than traditional security appliances. This allows packet decisions to be made based on more than just source/destination IP Address or ports and can also use information spanning across multiple connections for any given host. Network-based application firewalls operate at the application layer of a and can understand certain applications and protocols such as (FTP), (DNS), or (HTTP). This allows it to identify unwanted applications or services using a non standard port or detect if an allowed protocol is being abused. Modern versions of network-based application firewalls can include the following technologies: Web application firewalls (WAF) are a specialized version of a network-based appliance that acts as a , inspecting traffic before being forwarded to an associated server. A host-based application firewall monitors application or other general system communication. This gives more granularity and control, but is limited to only protecting the host it is running on. Control is applied by filtering on a per process basis. Generally, prompts are used to define rules for processes that have not yet received a connection. Further filtering can be done by examining the process ID of the owner of the data packets. Many host-based application firewalls are combined or used in conjunction with a packet filter. Due to technological limitations, modern solutions such as are being used as a replacement of host-based application firewalls to protect system processes. There are various application firewalls available, including both free and open source software and commercial products. Starting with Mac OS X Leopard, an implementation of the TrustedBSD MAC framework (taken from FreeBSD), was included. The TrustedBSD MAC framework is used to sandbox services and provides a firewall layer, given the configuration of the sharing services in X Leopard and Snow Leopard. Third-party applications can provide extended functionality, including filtering out outgoing connections by app. This is a list of security software packages for Linux, allowing filtering of application to OS communication, possibly on a by-user basis: These devices may be sold as hardware, software, or virtualized network appliances.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_url_1"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9e96d138-6195-45c7-adaa-50f9209d2678\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_url</th>\n",
       "      <th>scraped_url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Special:MyTalk</td>\n",
       "      <td>This user is currently blocked.\\nThe latest bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Spe...</td>\n",
       "      <td>edits articles recent contributors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:File_u...</td>\n",
       "      <td>Thank you for offering to contribute an image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Special:MyContri...</td>\n",
       "      <td>This IP address is currently blocked.\\nThe lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Contac...</td>\n",
       "      <td>How to report a problem with an article, or fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e96d138-6195-45c7-adaa-50f9209d2678')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9e96d138-6195-45c7-adaa-50f9209d2678 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9e96d138-6195-45c7-adaa-50f9209d2678');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-0d5bcfa4-2bed-4958-854d-9c4c2dc49339\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d5bcfa4-2bed-4958-854d-9c4c2dc49339')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-0d5bcfa4-2bed-4958-854d-9c4c2dc49339 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                   origin_url  \\\n",
       "0  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "1  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "2  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "5  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "6  https://en.wikipedia.org/wiki/Web_scraping   \n",
       "\n",
       "                                         scraped_url  \\\n",
       "0       https://en.wikipedia.org/wiki/Special:MyTalk   \n",
       "1  https://en.wikipedia.org/w/index.php?title=Spe...   \n",
       "2  https://en.wikipedia.org/wiki/Wikipedia:File_u...   \n",
       "5  https://en.wikipedia.org/wiki/Special:MyContri...   \n",
       "6  https://en.wikipedia.org/wiki/Wikipedia:Contac...   \n",
       "\n",
       "                                                text  \n",
       "0  This user is currently blocked.\\nThe latest bl...  \n",
       "1                 edits articles recent contributors  \n",
       "2  Thank you for offering to contribute an image ...  \n",
       "5  This IP address is currently blocked.\\nThe lat...  \n",
       "6  How to report a problem with an article, or fi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmKuiuNXvoO3"
   },
   "outputs": [],
   "source": [
    "df_url_1['text'] = df_url_1['text'].str.strip()\n",
    "df_url_1 = df_url_1[df_url_1['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCZL6rZfv2mO"
   },
   "outputs": [],
   "source": [
    "df_url_2['text'] = df_url_1['text'].str.strip()\n",
    "df_url_1 = df_url_1[df_url_1['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uv68SbkJu3Aj",
    "outputId": "0ed40147-b2f8-44b0-fc67-93f790badc36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 3)\n",
      "(274, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_url_1.shape)\n",
    "print(df_url_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gC7rs_zHzhe5"
   },
   "outputs": [],
   "source": [
    "# Again, below is a version of the above code that will implement logging\n",
    "\"\"\"\n",
    "# Step 1: Install necessary packages and start TOR\n",
    "!apt-get install -y tor > /dev/null 2>&1\n",
    "!pip install stem > /dev/null 2>&1\n",
    "!pip install pysocks > /dev/null 2>&1\n",
    "!pip install pandas > /dev/null 2>&1\n",
    "!pip install scrapy > /dev/null 2>&1\n",
    "\n",
    "import time\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "import subprocess\n",
    "import socks\n",
    "import socket\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Create the torrc file\n",
    "torrc_content = \"\"\"\n",
    "ControlPort 9051\n",
    "CookieAuthentication 0\n",
    "SocksPort 9050\n",
    "Log notice file /var/log/tor/notices.log\n",
    "\"\"\"\n",
    "with open('torrc', 'w') as f:\n",
    "    f.write(torrc_content)\n",
    "\n",
    "# Step 3: Start TOR using subprocess and wait for it to be ready\n",
    "tor_process = subprocess.Popen(['tor', '-f', 'torrc'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Give TOR some time to start\n",
    "time.sleep(20)\n",
    "\n",
    "# Step 4: Renew TOR identity\n",
    "def renew_connection():\n",
    "    try:\n",
    "        with Controller.from_port(port=9051) as controller:\n",
    "            controller.authenticate()\n",
    "            controller.signal(Signal.NEWNYM)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to renew TOR connection: {e}\")\n",
    "\n",
    "# Step 5: Create a list of user predetermined URLs to scrape\n",
    "urls_to_scrape = [\n",
    "    'https://en.wikipedia.org/wiki/Web_scraping',\n",
    "    'https://en.wikipedia.org/wiki/Data_scraping',\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Assign unique IDs to URLs\n",
    "url_id_map = {url: f\"url_{i+1}\" for i, url in enumerate(urls_to_scrape)}\n",
    "\n",
    "# Step 6: Scrapy Spider\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Set up SOCKS5 proxy for Scrapy using PySocks\n",
    "socks.set_default_proxy(socks.SOCKS5, \"127.0.0.1\", 9050)\n",
    "socket.socket = socks.socksocket\n",
    "\n",
    "# Initialize the dictionary outside of the Spider class\n",
    "data_dicts = {}\n",
    "\n",
    "# Create the Spider class\n",
    "class DCChapterSpider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        renew_connection()  # Renew TOR identity before starting requests\n",
    "        for url, unique_id in url_id_map.items():\n",
    "            yield scrapy.Request(url=url, callback=self.parse1, meta={'unique_id': unique_id, 'origin_url': url})\n",
    "\n",
    "\n",
    "    def parse1(self, response):\n",
    "        unique_id = response.meta['unique_id']\n",
    "        origin_url = response.meta['origin_url']\n",
    "        links = response.xpath('//a/@href').extract()\n",
    "        self.log(f\"Found {len(links)} links on the page\")\n",
    "        for link in links:\n",
    "            absolute_url = response.urljoin(link)\n",
    "            # Ensure to follow the link even if it appears similar to the origin_url\n",
    "            if absolute_url != origin_url:\n",
    "                yield scrapy.Request(url=absolute_url, callback=self.parse2, meta={'unique_id': unique_id, 'scraped_url': absolute_url, 'origin_url': origin_url})\n",
    "\n",
    "\n",
    "    def parse2(self, response):\n",
    "        unique_id = response.meta['unique_id']\n",
    "        scraped_url = response.meta['scraped_url']\n",
    "        origin_url = response.meta['origin_url']\n",
    "        # Correct XPath selector for extracting paragraph text\n",
    "        par_text = response.xpath('//p/text()').extract()\n",
    "        self.log(f\"Processing URL: {response.url}\")\n",
    "        self.log(f\"Found {len(par_text)} paragraphs on the page\")\n",
    "        par_text_strip = [t.strip() for t in par_text if t.strip()]  # Remove empty strings\n",
    "        # Store the data in the dictionary with unique_id as the key\n",
    "        if unique_id not in data_dicts:\n",
    "            data_dicts[unique_id] = []\n",
    "        data_dicts[unique_id].append({\n",
    "            'origin_url': origin_url,\n",
    "            'scraped_url': scraped_url,\n",
    "            'text': ' '.join(par_text_strip)  # Concatenate all paragraph text into a single string\n",
    "        })\n",
    "        self.log(f\"Stored data for URL: {response.url}\")\n",
    "\n",
    "# Configure Scrapy to use the SOCKS proxy\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCChapterSpider)\n",
    "process.start()\n",
    "\n",
    "# Step 7: Process the collected data and convert each dictionary to a DataFrame\n",
    "dataframes = {}\n",
    "\n",
    "for unique_id, entries in data_dicts.items():\n",
    "    # Create a DataFrame from the list of entries\n",
    "    df = pd.DataFrame(entries)\n",
    "    # Store the DataFrame with unique ID in the name\n",
    "    dataframes[f\"df_{unique_id}\"] = df\n",
    "\n",
    "# Step 8: Display the DataFrames\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"DataFrame: {df_name}\")\n",
    "    print(df.head())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcockhWVx3j5"
   },
   "source": [
    "## **Comparison of Both Implementations:**\n",
    "\n",
    "### The two web scraping implementations are similar in that they both use the TOR network to anonymize requests, utilize the Scrapy framework for scraping, and process the collected data into Pandas DataFrames. However, there are key differences between them in terms of their structure and functionality. Here is a breakdown of the differences:\n",
    "\n",
    "<br>\n",
    "\n",
    "## ***Similarities:***\n",
    "\n",
    "<br>\n",
    "\n",
    "### - **TOR Integration:** Both codes set up and start a TOR process, use PySocks to route requests through TOR, and have a function to renew the TOR identity.\n",
    "\n",
    "### - **Scrapy Usage:** Both use the Scrapy framework to define a spider for scraping data from web pages.\n",
    "\n",
    "\n",
    "### - **Data Processing:** Both process scraped data using Pandas DataFrames to clean and structure the data.\n",
    "\n",
    "<br>\n",
    "\n",
    "## ***Differences:***\n",
    "\n",
    "<br>\n",
    "\n",
    "### - **URL Handling:**\n",
    "\n",
    "### **First Code:** It starts scraping from a single hardcoded URL (https://en.wikipedia.org/wiki/Web_scraping) and follows links from that page. The data collected from each followed link is stored in a single dictionary, data_link_dict.\n",
    "\n",
    "### **Second Code:** It uses a predefined list of URLs (urls_to_scrape) to start scraping. Each URL is assigned a unique ID (url_id_map), and data from each URL is stored in a separate dictionary under the unique ID, resulting in a more organized structure for handling multiple starting URLs.\n",
    "\n",
    "<br>\n",
    "\n",
    "### - **Data Storage:**\n",
    "\n",
    "### **First Code:** Uses a single dictionary (data_link_dict) to store all scraped data, with URLs as keys and lists of paragraph texts as values.\n",
    "\n",
    "### **Second Code:** Uses a nested dictionary (data_dicts) where each unique URL ID maps to another dictionary, which then maps base URLs to lists of paragraph texts. This allows for clearer data organization when dealing with multiple initial URLs.\n",
    "\n",
    "<br>\n",
    "\n",
    "### - **DataFrame Creation:**\n",
    "\n",
    "### **First Code:** Converts the single dictionary (data_link_dict) into one DataFrame after cleaning and standardizing the data.\n",
    "\n",
    "### **Second Code:** Creates a separate DataFrame for each unique URL ID. Each DataFrame corresponds to one of the initial URLs and is stored in a dictionary (dataframes), allowing for the handling and analysis of data on a per-URL basis.\n",
    "\n",
    "<br>\n",
    "\n",
    "### - **Scrapy Spider Logic:**\n",
    "\n",
    "### **First Code:** The spider starts by requesting a single hardcoded URL and then follows links from that page.\n",
    "\n",
    "### **Second Code:** The spider starts by iterating over a list of predefined URLs, making it more flexible for scraping multiple specific pages without modifying the code for each new URL.\n",
    "\n",
    "<br>\n",
    "\n",
    "### - **Data Display:**\n",
    "\n",
    "### **First Code:** Displays a single DataFrame containing all scraped data.\n",
    "\n",
    "### **Second Code:** Iterates over each DataFrame in the dataframes dictionary, displaying them separately with unique identifiers, facilitating the distinction of data from different starting URLs.\n",
    "\n",
    "<br>\n",
    "\n",
    "### In summary, the second code is more flexible and organized, especially when dealing with multiple starting URLs. It structures data in a way that allows for easier analysis on a per-URL basis, whereas the first code is simpler and suitable for single-page scraping with link-following."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
